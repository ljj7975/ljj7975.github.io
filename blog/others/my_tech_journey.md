## Tracing My Steps in Tech - #1 Where Is Technology Heading Next?

### TL;DR: "The past is a prologue." — _William Shakespeare (from The Tempest)_. Maybe. Maybe not. But revisiting my journey might offer some useful insights into what lies ahead.

Although I never imagined studying computer science, I ended up majoring in it at university, despite having only a vague idea of what it entailed at the time of admission. Thanks to the guidance of many insightful seniors during my university years, I am now working in one of the hottest fields: AI, pushing the boundaries of innovation as a researcher. Whether by intention or luck, the choices I've made over the past ten years have brought me to a place I’m truly grateful for.

Now that my personal life is more settled than ever, I often ponder what lies ahead. Among various thoughts, I frequently find myself contemplating the future of the IT field, in which I am deeply involved. Despite spending considerable time thinking about what the next big thing might be, each session ends with different and inconclusive answers, prompting me to reflect on my journey. Thankfully, I’ve consistently been a few years ahead, studying the next big thing just before it became mainstream. Revisiting my path as a software engineer and researcher might provide some insights into the future of IT.

Additionally, I want to consider the mindsets I’ve held at different stages of my career. Recently, I've come to appreciate that mindset and motivation are crucial in everything I do. With a long road still ahead, I want to ensure I use my time in the most meaningful way possible. By the end of this series, I hope to have a concrete answer to the big question: where is IT heading?

## Tracing My Steps in Tech - #2 How I Began My Career with Web Development

### TL;DR: In general, web development has the largest market and the lowest barrier to entry, which is why most developers start their careers in this field.

It was indeed a coincidence how I ended up selecting computer science as my major (there’s a story behind it, but I'll save it for another time). When I first enrolled, my understanding of computer science was quite limited. However, I quickly discovered that the problem-solving and logical thinking involved in programming was something I not only enjoyed but also had a natural aptitude for.

The first industry-focused skills I developed were in web development. As a student at the University of Waterloo, I was required to complete multiple internships before graduating. When I entered the internship market for the first time, I quickly realized that my competitors weren’t just other students with similar expertise and knowledge, my real contenders were seniors with more experience. Recognizing this, I understood that securing an internship had to be my top priority, rather than holding out for the perfect opportunity. To increase my chances, I decided to aim for a high-demand field: web development. Many developers begin their careers in front-end development because it typically requires less knowledge of the system's overall infrastructure.

My first internship was at a startup called Mozzaz Corp., where I learned the basics of web development with a focus on front-end. I had a great time at Mozzaz, but I vividly remember the challenges our team faced when working with designers. They often requested changes that seemed simple, like relocating a component, but were quite difficult to implement. This experience made me realize that I preferred working on projects where the stakeholders are other developers--people who are already familiar with the engineering effort involved.

## Tracing My Steps in Tech - #3 My Journey into Big Data and Deep Learning

### TL;DR: Despite the emphasis on deep learning, I argue that we are still predominantly in the era of Big Data.

The next two years went smoothly and were filled with enjoyable experiences. I made unforgettable memories with colleagues while immersing myself in fascinating new knowledge. I built a solid foundation in computer science through my courses and gained valuable hands-on experience through multiple internships. As my perspective on software engineering broadened, I began to recognize emerging trends and the overall direction of technological advancements. One trend that particularly caught my attention was Big Data.

In 2016, I enrolled in a course titled "Big Data Infrastructure." The first lecture, which covered the origins of Big Data, was not only insightful but also memorable, especially since it was when I first met Jimmy Lin, who later supervised me throughout my master’s program. I still recall most of the content from that lecture, as I have repeatedly shared it with my colleagues since then.

Big Data emerged from the success of social networks (SNS). In the early 2010s, platforms like Twitter and Facebook harnessed the true potential of the internet, establishing global user bases and generating an immense amount of data. At that time, computational power was insufficient to process all this data effectively, so companies opted to store it first and address its utilization later. Over the years, advancements in hardware enabled these companies to process the data more efficiently and extract meaningful insights. This shift sparked increased interest and efforts in developing infrastructure capable of handling large volumes of data and deriving valuable information to benefit businesses, effectively closing the loop.

As time went on, I learned about the development of deep learning, which is closely connected to the Big Data story. The concept of neural networks and their mathematical foundations have been explored since the 1980s. However, achieving practical results was difficult due to limited computational power and inadequate data. Much like the evolution of Big Data, these ideas were revisited and realized as advancements in technology progressed and more data became available.

Understanding the origins of these trends clarified the future trajectory of technology. The tools I explored in the Big Data course, such as Spark and Hadoop, were particularly eye-opening, as their fundamental operations differed significantly from traditional sequential processes. I also enjoyed seeing how Big Data could transform vast amounts of seemingly meaningless data into valuable insights.

In my final year of university, I focused on deepening my expertise in this area. I gained hands-on experience with various large-scale data processing tools, including Spark and Hadoop, as well as deep learning frameworks like Keras and TensorFlow. I particularly enjoyed my project at Uber, which involved integrating TensorFlow with Spark to enable advanced data analysis through deep learning within a Spark-based infrastructure.

I firmly believe that we are still very much in the era of Big Data. Although AI, particularly deep learning, is a prominent theme today, its advancements would not be possible without the foundational progress made in Big Data. I am truly grateful for the many insightful individuals I encountered during my undergraduate years. Their guidance played a crucial role in shaping my journey and helping me achieve the successes that I am proud of today.

## Tracing My Steps in Tech - #4 My Specialization in Deep Learning Research

### TL;DR: The latest trends in AI, particularly in prompt engineering, stem from the motivation to achieve efficient domain adaptation—-a broader topic that my research during my Master’s program addressed.

Reflecting on my time during my Master’s program, I initially concentrated heavily on gaining hands-on experience with deep learning through various projects. Although this approach may have led to a lack of specialization in a particular topic, it proved less problematic than I had anticipated. This is because all my projects ultimately aligned with the same goal, which was also the focus of our lab: efficient domain adaptation. Specifically, my research during this period centered on enhancing user experience while minimizing deployment efforts.

My first few academic papers centered on [Honkling](https://aclanthology.org/D19-3016/), the first JavaScript-based keyword spotting system. The motivation behind this research stemmed from two key realizations: 1) JavaScript’s crucial role in building user-facing applications, not only for web pages but also for standalone applications running on handheld devices, and 2) the growing privacy concerns with speech recognition, which underscore the need for on-device processing to avoid unnecessary personal data sharing. The final version of Honkling not only enabled keyword spotting functionality in a JavaScript-based system but also supported on-device tuning with a few user-spoken phrases, further enhancing the user experience. I’m particularly proud of this project and its successor, [Howl](https://aclanthology.org/2020.nlposs-1.9/), which was eventually deployed as part of Firefox Voice. I am surprised by how much value it has generated in the community, as I still receive consulting requests regarding these topics now and then.

During the latter half of my Master’s program, I concentrated more on textual data, specifically by using a single language model for various tasks and languages. The groundbreaking Bidirectional Encoder Representations from Transformers (BERT) demonstrated that fine-tuning a foundational model with general knowledge for a specific domain can yield solutions comparable to those developed for particular tasks within a narrower scope. While my primary focus was on studying the flexibility of BERT and [the impact of freezing layers during training](https://ljj7975.github.io/assets/papers/freezing.pdf), I also collaborated with a colleague on a project called [DeeBERT](https://aclanthology.org/2020.acl-main.204/), which aimed to reduce computational requirements during inference without compromising performance.

Interestingly, the experience and knowledge I gained in efficient domain adaptation during this time have been crucial for understanding the latest trends in AI, particularly in prompt engineering. I now recognize that my expertise is deeply rooted in this area, which I consider my specialty. This is closely related to the research I am currently working on, and having a clear understanding of the underlying motivations often helps me grasp the bigger picture in my work. Recognizing the importance of grasping these fundamentals to understand the direction of prompt engineering, I have made additional efforts to summarize my findings and insights in [a separate blog post](https://github.com/ljj7975/ljj7975.github.io/tree/main/blog/research#what-is-the-focus-of-my-research-at-epsons-cv-and-robotics-lab).

## Tracing My Steps in Tech - #5 Designing Technology for Greater Impact: Lessons Learned from My Journey in CV and Robotics

### TL;DR: Building trust with users is essential but challenging for deep learning solutions due to their probabilistic nature and black-box characteristics. Bridging this gap requires both method-specific and modality-specific approaches.

I am genuinely interested in deep learning as a methodology rather than the specific advancements it brings to a particular domain. However, I have come to realize that the underlying modality can introduce different challenges, leading to vastly different research experiences. For example, one significant challenge I encountered with speech recognition was the limited explainability due to the difficulty in interpreting the model's intermediate representations. Acoustic data, the input modality, must be converted into numeric data for deep learning. Analyzing the model’s inner workings often proved challenging and sometimes futile, as the intermediate numeric data derived from acoustic signals could not be easily translated back into a form understandable to humans. In contrast, visual data provides more opportunities for interpreting the model’s inner workings through the visualization of intermediate activations, a well-studied area known for its effectiveness and ease of interpretability.

Recognizing that the underlying domain can significantly impact my experience with deep learning, I became eager to explore other modalities. Consequently, when it was time to decide whether to pursue a PhD, I realized that identifying the exact modality which I would be happiest working was crucial. Therefore, I decided to embark on a new journey to discover what truly excites me.

With this mindset and considering my personal situation at the time, I transitioned to a startup specializing in bin-picking solutions based on 3D object detection. For those unfamiliar with bin picking, it involves using a robot arm to move objects by accurately locating them with a camera. As a research scientist, I collaborated with colleagues specializing in computer vision (CV) to develop a production-ready solution, gaining hands-on experience with various algorithms in both CV and robotics. Despite the substantial shift from my background in textual and acoustic data, I found the new learning experience to be extremely rewarding.

While existing academic solutions can be combined to create an effective bin-picking pipeline, a significant gap remains in achieving a commercially viable system, especially for ambitious goals like operating bin-picking processes 24/7 without human intervention. Our team made significant advancements beyond these existing solutions, ultimately developing an accurate, real-time pipeline that meets demanding user requirements. The pipeline starts with a depth completion technique to address sensor failures. It then employs a 2D object detection algorithm to estimate the potential location of each object. Next, the 6D pose estimation algorithm determines the precise location and orientation of each object. Traditional template matching algorithms are then used to further refine the poses based on local point cloud data. The subsequent step involves estimating the optimal grasping position for each object. Grasping feasibility and detection confidence are combined into a single score, and the optimal option is selected based on this score. Finally, the trajectory from the robot arm’s current location to the selected object is computed, and the robot arm is instructed to pick up the object and place it at a specified location with the desired orientation.

During development, I applied my extensive software development expertise to design our product with a strong focus on user experience. My user-centric approach and diverse experience across multiple software companies led to my promotion to a lead role. This role involved not only guiding the direction of our core technology but also managing an international team of engineers and overseeing the deployment of our product to multiple customers. This leadership role was a new experience for me, requiring skills beyond the technical. One of the most important skills I developed was communication, as I often needed to explain our achievements and advantages to non-technical stakeholders in an accessible manner. Additionally, I honed my prioritization skills, learning to evaluate different options accurately based on available resources. With responsibilities similar to those of a CTO, I also became hands-on with the overall operation of the business. This experience was enlightening, giving me a deeper understanding of the perspective gaps among management, as product sellers; engineers, as developers of the core technology; and customers, as end users of the product.

Overall, this period was crucial in shaping my view on how technology should be designed and implemented to truly benefit users. If I had to distill this perspective into a single word, it would be "trust". For software to be effectively integrated into daily life, building trust with users is crucial. This trust can be established in several ways, such as by reliably performing the intended tasks to the user's satisfaction or by providing clear evidence of the reasoning behind its operations.

Specifically, I believe that deep learning often falls short in this regard, especially when dealing with non-expert users. As a statistical model, it can fail unexpectedly, and as a black box, it provides only limited explanations regarding its decisions. Addressing this issue would require not only method-specific solutions but also modality-specific approaches, which ties back to the initial point I made about the unique challenges and experiences arising from different underlying modalities.

## Tracing My Steps in Tech - #6 Research Experience at Epson: Effective Explainability and Efficient Domain Adaptation Using Foundation Models

### TL;DR: At Epson, I am working on effective methods to analyze the knowledge embedded in Vision Language Models to enhance usability, as I believe this is essential for making deep learning accessible to users without a technical background.

While managing a production-ready application and refining it based on stakeholder feedback was highly rewarding, I longed to be more hands-on with the technology itself. I was eager to align my daily work with research-oriented tasks, identifying fundamental challenges in deep learning and exploring new advancements through in-depth analysis. Consequently, I transitioned to Epson's CV and Robotics labs, where I could focus more on innovation rather than on business-oriented tasks.

When I joined Epson in 2023, the organization was shifting its focus from bin-picking to Vision Language Models, staying abreast of the latest technological advancements. Our team is dedicated to interpreting the internal knowledge captured by foundational models (FMs) and using it effectively to solve specific user tasks. We have found that explainability is key to building trust with users, and prompt engineering offers significant benefits for efficient domain adaptation. In terms of explainability, our efforts concentrate on ensuring that the model accurately captures concepts and provides a way to verify the model’s understanding in a manner that is interpretable by users. This is crucial because, without a solid grasp of concepts, task-specific solutions built on FMs may merely memorize training samples, making them sensitive to noise. Our efforts in efficient domain adaptation also aim to enhance user experience by accurately interpreting user intentions from minimal interaction, ensuring the technology is accessible and beneficial for all users, including non-experts.

I find my research experience at Epson to be both unique and meaningful. As an independent researcher, I have the freedom to plan and execute my research autonomously. I present my progress weekly to management and experienced colleagues, ensuring that my work remains aligned with our original goals and fundamental questions while aiming to deliver tangible benefits. This regular guidance and fruitful discussions have significantly enhanced my skills as a researcher, helping me identify what to focus on and how to avoid distractions. Many of these discussions on fundamental questions have also been the source of my latest thoughts, which I have documented in [previous blog posts](https://github.com/ljj7975/ljj7975.github.io/tree/main/blog/research).

This summarizes my journey as a software engineer and deep learning researcher. Reflecting on my hands-on experiences across various domains, I see how they have converged to shape my current responsibilities. Looking back, the driving factor in technology has always been user convenience, or in other words, usability. Technology evolves to benefit end users, particularly those with limited technical understanding. I firmly believe this trend will continue, with advancements aimed at reducing technical barriers regardless of users' prior domain knowledge.
