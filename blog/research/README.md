## What is my interest?

### TL;DR: If we cannot interpret it, we cannot draw any conclusions and therefore cannot use it.

About a year ago, I began working on few-shot object detection and classification. Throughout my research, I have constantly wondered whether the model truly understands the concept or simply memorizes it based on the examples, especially when reviewing the improved detection accuracy. This question has shaped the direction of my research, but I struggled to clearly describe the end goal due to its dependency on multiple interconnected components. Recently, I had engaging conversations with several very smart people about AI, which helped me finally articulate this in a clear sentence.

**I define intelligence as the ability to extract abstract concepts from knowledge (comprehension) and adapt to a new domain (transferability)**. In other words, I argue that an agent is intelligent only if it demonstrates both comprehension and transferability. This is how we learn and convey our understanding.

As an individual with a research-oriented mindset and a solid understanding of industry priorities, **my personal interest lies in studying how to build an intelligent agent that demonstrates its intelligence in an interpretable form, ultimately benefiting humans**.

**I believe information sharing across different modalities plays a crucial role in building an intelligent agent by enabling the alignment between representation of human knowledge and representation of model knowledge**. (Although I strongly believe that data-driven and statistical methods introduce bias,) I think single-modality models may already possess intelligence, but it is represented in a way that humans cannot interpret. When asked if latest vision language models are intelligent, people often give different answers because we tend to interpret the model's knowledge from a human perspective, assuming the model represents knowledge in the same way humans do (e.g., visualizing attention maps and checking if they fall on reasonable pixels).

I believe that the knowledge obtained by a model from single-modality data would never be interpretable by humans. However, the latest advancements in AI, which aim to align multiple modalities in a human-interpretable fashion through similarity-based learning objectives, have indicated the direction I need to pursue in building an intelligent agent: If my goal is to create an intelligent agent that perceives and interprets the world like humans do, the knowledge representation must at least be aligned with that of humans.

One excellent example of this is Explainable AI designed around visualizing cross-attention between text and image inputs. When self-attentions from a vision-only model are visualized, the groups of similar pixel features do not make much sense to humans. However, with text-guided attention introduced, pixels correlated with the provided text show higher scores, enabling humans to understand where the model is focusing. I see this as the text modality serving as an interface for interpreting the model's knowledge represented in the visual modality. In other words, a vision-only model becomes more accessible to non-expert users through a text-based interface.

Overall, my research goal can be (finally) summarized as **building an intelligent agent that possesses human-interpretable comprehension ability and demonstrates its benefits to humanity through effective transferability**.

(As you might have inferred from my previous experiences,) I have had a genuine interest in deep learning itself rather than its domain or modality-specific applications since the beginning of my career. After solidifying the type of research I want to pursue, I believe this perspective has helped me establish the aforementioned view on AI significantly. More importantly, while I have previously thought my diverse experience might lack specialization, I now see that my experiences align well with my future direction.

## What is the focus of my research at Epson's CV and Robotics Lab?

### TL;DR: The future lies in developing user-friendly AI solutions that accurately understand user intent with minimal interaction and resources.

Traditionally, designing task-specific networks was the standard approach to solving problems with deep learning. At that time, there was limited interest in creating a unified model capable of supporting multiple tasks due to training difficulties and the limited per-task performance, which stemmed from the confusion caused by the entanglement of internal knowledge. However, with the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language processing (NLP), everything has changed. Although there were prior efforts to build foundational language models with generic text interpretation abilities, BERT was the first model to demonstrate outstanding adaptability across a large number of NLP tasks, outperforming most existing task-specific solutions while requiring relatively less training effort. It has shifted the paradigm in deep learning by showing that a foundation model with strong generic knowledge can learn specific tasks very efficiently while outperforming models explicitly designed for specific tasks. It was a clear signal that the future of AI would heavily rely on this paradigm.

Today, I view prompt engineering as a natural extension of this paradigm. It pushes the concept of reducing the effort needed to build task-specific solutions to the extreme, sometimes at the cost of performance. Yet, this trade-off is often worthwhile, as evidenced by the success of applications like ChatGPT. However, this new wave of AI brings its own set of challenges. As foundation models expand to encompass all the knowledge needed to handle a growing array of tasks, they require more resources and have become monopolized by big tech companies with substantial funding, resulting in only a handful of public solutions.

This resource barrier leaves less well-funded groups with limited research options, the most obvious being tuning these foundation models for specific tasks rather than enhancing the generic knowledge of the models themselves. At Epson's CV and Robotics Lab, where I currently work, we face similar challenges. Due to limited resources, we struggle to conduct research in building custom foundation models and often rely on leveraging existing models. However, this does not stop us from pursuing innovation. When our lab discusses future directions, we spend considerable time exploring how we can provide unique advancements that benefit individuals in meaningful ways while keeping pace with the latest AI trends.

Our view on the aforementioned paradigms is as follows: While these intelligent foundation models possess vast amounts of generic knowledge, there will always be gaps when it comes to executing specific tasks that users need. The interaction required can be substantial, particularly for complex tasks. Therefore, similar to the original motivation behind prompt engineering, we find value in offering effective solutions for tuning these foundational models to specific user interests. We aim to minimize the resources and effort required while also making it accessible to non-expert users.

To minimize the necessary tuning effort and enhance user experience, it is crucial to use the appropriate modality for communication. For instance, if the task is in the domain of computer vision (CV), communication based on videos or images would be more effective, while textual explanations and instructions are better suited for NLP tasks. In this direction, we are exploring multimodal foundation models to further leverage knowledge constructed across multiple modalities to minimize the resources that need to be explicitly provided by the user during tuning. These resources include not only data but also computational power, with the amount required varying based on task complexity, data quality, and the existing capabilities of the foundational model. As we grapple with the inherent challenges of deep learning—particularly the issue of limited interpretability—we recognize the importance of Explainable AI (XAI) in enhancing the overall user experience. My current research lies at the intersection of these topics, uncovering new ways to combine the strengths of these approaches into something truly groundbreaking.

The journey is far from over, and the potential is immense. I’m excited to see where the next wave of innovation will take me.
